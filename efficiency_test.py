import pandas as pd
import time
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from transformers import BertTokenizerFast, BertForSequenceClassification
import torch
import os
import re
from collections import Counter, defaultdict
import warnings

warnings.filterwarnings('ignore')


# è®¾ç½®ä¸­æ–‡å­—ä½“ï¼Œè§£å†³ä¸­æ–‡æ˜¾ç¤ºé—®é¢˜
def setup_chinese_font():
    """è®¾ç½®ä¸­æ–‡å­—ä½“ï¼Œè§£å†³ä¸­æ–‡æ˜¾ç¤ºé—®é¢˜ï¼ˆä¼˜åŒ–ç‰ˆï¼‰"""
    print("\nğŸ”¤ å¼€å§‹é…ç½®ä¸­æ–‡å­—ä½“...")
    # å®šä¹‰å¸¸è§ä¸­æ–‡å­—ä½“åç§°å’Œå¯èƒ½çš„ç³»ç»Ÿè·¯å¾„
    chinese_font_names = [
        'SimHei', 'Microsoft YaHei', 'Microsoft YaHei UI',
        'PingFang SC', 'Songti SC', 'KaiTi SC',
        'WenQuanYi Zen Hei', 'DejaVu Sans'
    ]

    # å¸¸è§ä¸­æ–‡å­—ä½“æ–‡ä»¶è·¯å¾„
    common_font_paths = [
        'C:/Windows/Fonts/simhei.ttf',  # Windows
        'C:/Windows/Fonts/msyh.ttc',  # Windows
        'C:/Windows/Fonts/msyhl.ttc',  # Windows
        '/Library/Fonts/PingFang SC.ttc',  # macOS
        '/usr/share/fonts/wenquanyi/wqy-zenhei/wqy-zenhei.ttc'  # Linux
    ]

    try:
        import matplotlib.font_manager as fm
        # å…ˆå°è¯•é€šè¿‡å­—ä½“åç§°è®¾ç½®
        available_fonts = [f.name for f in fm.fontManager.ttflist]
        target_font = None

        # æŸ¥æ‰¾å¯ç”¨çš„ä¸­æ–‡å­—ä½“
        for font_name in chinese_font_names:
            if font_name in available_fonts:
                target_font = font_name
                break

        if target_font:
            plt.rcParams['font.sans-serif'] = [target_font] + ['DejaVu Sans']
            plt.rcParams['axes.unicode_minus'] = False
            print(f"âœ… æˆåŠŸä½¿ç”¨ç³»ç»Ÿå­—ä½“: {target_font}")
            return

        # è‹¥æœªæ‰¾åˆ°æ³¨å†Œå­—ä½“ï¼Œå°è¯•æ‰‹åŠ¨åŠ è½½å­—ä½“æ–‡ä»¶
        for font_path in common_font_paths:
            if os.path.exists(font_path):
                fm.fontManager.addfont(font_path)
                font_prop = fm.FontProperties(fname=font_path)
                font_name = font_prop.get_name()

                plt.rcParams['font.sans-serif'] = [font_name] + ['DejaVu Sans']
                plt.rcParams['axes.unicode_minus'] = False
                print(f"âœ… æ‰‹åŠ¨åŠ è½½å­—ä½“æˆåŠŸ: {font_name} (è·¯å¾„: {font_path})")
                return

        # æ‰€æœ‰æ–¹æ³•å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å­—ä½“å¹¶æç¤º
        print("âš ï¸ æœªæ‰¾åˆ°ä»»ä½•ä¸­æ–‡å­—ä½“ï¼Œå°†ä½¿ç”¨é»˜è®¤å­—ä½“ï¼ˆä¸­æ–‡å¯èƒ½æ˜¾ç¤ºä¸ºæ–¹æ¡†ï¼‰")
        print("ğŸ’¡ å»ºè®®æ‰‹åŠ¨å®‰è£…å­—ä½“ï¼š")
        print("   - Windows: å®‰è£… SimHeiï¼ˆé»‘ä½“ï¼‰æˆ– Microsoft YaHeiï¼ˆå¾®è½¯é›…é»‘ï¼‰")
        print("   - macOS: ç¡®ä¿ PingFang SCï¼ˆè‹¹æ–¹ï¼‰å·²å¯ç”¨")
        print("   - Linux: å®‰è£… WenQuanYi Zen Heiï¼ˆæ–‡æ³‰é©¿æ­£é»‘ï¼‰")

    except Exception as e:
        print(f"âŒ å­—ä½“è®¾ç½®è¿‡ç¨‹å‡ºé”™: {str(e)}")
        # å‡ºé”™åå¼ºåˆ¶è®¾ç½®åŸºç¡€å‚æ•°
        plt.rcParams['font.sans-serif'] = ['DejaVu Sans', 'Arial Unicode MS']
        plt.rcParams['axes.unicode_minus'] = False


# åˆå§‹åŒ–å­—ä½“
setup_chinese_font()
sns.set_style("whitegrid")

# è¯ˆéª—ç±»å‹é…ç½®
LABEL_MAP = {0: "æ­£å¸¸å¯¹è¯", 1: "åˆ·å•è¿”åˆ©", 2: "è™šå‡æŠ•èµ„", 3: "å†’å……å®¢æœ"}
REVERSE_LABEL_MAP = {v: k for k, v in LABEL_MAP.items()}

# å¤§ç±»æ˜ å°„ï¼šæ­£å¸¸ vs è¯ˆéª—
BINARY_MAP = {
    "æ­£å¸¸å¯¹è¯": "æ­£å¸¸",
    "åˆ·å•è¿”åˆ©": "è¯ˆéª—",
    "è™šå‡æŠ•èµ„": "è¯ˆéª—",
    "å†’å……å®¢æœ": "è¯ˆéª—"
}


class HumanVsModelFinal:
    def __init__(self, model_path, tokenizer_path):
        self.model_path = model_path
        self.tokenizer_path = tokenizer_path
        self.model = None
        self.tokenizer = None
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        print(f"ğŸ”§ ä½¿ç”¨è®¡ç®—è®¾å¤‡: {self.device}")

    def load_model(self):
        """åŠ è½½æ¨¡å‹"""
        print("\nğŸ“¥ å¼€å§‹åŠ è½½æ¨¡å‹...")
        try:
            self.tokenizer = BertTokenizerFast.from_pretrained(self.tokenizer_path)
            self.model = BertForSequenceClassification.from_pretrained(
                self.model_path,
                num_labels=len(LABEL_MAP),
                ignore_mismatched_sizes=True
            )
            self.model.eval()
            self.model.to(self.device)
            print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼")
            return True
        except Exception as e:
            print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            return False

    def model_predict(self, text):
        """æ¨¡å‹é¢„æµ‹"""
        try:
            inputs = self.tokenizer(
                text,
                return_tensors="pt",
                truncation=True,
                padding="max_length",
                max_length=128
            ).to(self.device)

            with torch.no_grad():
                outputs = self.model(**inputs)
                logits = outputs.logits
                probs = torch.softmax(logits, dim=1).cpu().numpy()[0]

            pred_label = np.argmax(probs)
            pred_category = LABEL_MAP[pred_label]
            confidence = round(probs[pred_label] * 100, 2)

            return pred_label, pred_category, confidence

        except Exception as e:
            print(f"âŒ é¢„æµ‹å¤±è´¥: {e}")
            return -1, "é¢„æµ‹é”™è¯¯", 0

    def load_data_with_true_labels(self):
        """åŠ è½½å¸¦çœŸå®æ ‡ç­¾çš„æ•°æ®"""
        print("\nğŸ“ æ­£åœ¨åŠ è½½å¸¦æ ‡ç­¾çš„æ•°æ®...")
        data_files = ["data.csv", "preprocessed_scam_data.csv"]

        for file in data_files:
            if os.path.exists(file):
                try:
                    df = pd.read_csv(file)
                    if 'content' not in df.columns or 'label' not in df.columns:
                        print(f"âš ï¸ {file} ç¼ºå°‘contentæˆ–labelåˆ—")
                        continue

                    df = df.dropna(subset=['content', 'label'])
                    df = df[df['content'].str.len() >= 5]
                    df['label'] = df['label'].astype(int)

                    valid_labels = set(LABEL_MAP.keys())
                    df = df[df['label'].isin(valid_labels)]

                    if len(df) > 0:
                        print(f"âœ… ä» {file} åŠ è½½æ•°æ®: {len(df)} æ¡å¸¦æ ‡ç­¾æ ·æœ¬")
                        return df

                except Exception as e:
                    print(f"âŒ åŠ è½½ {file} å¤±è´¥: {e}")

        # ä½¿ç”¨ç¤ºä¾‹æ•°æ®
        print("âš ï¸ æœªæ‰¾åˆ°å¸¦æ ‡ç­¾æ•°æ®æ–‡ä»¶ï¼Œä½¿ç”¨ç¤ºä¾‹æ•°æ®")
        example_data = [
            {"content": "æ‚¨å¥½ï¼Œæˆ‘æ˜¯å¿«é€’å®¢æœï¼Œæ‚¨çš„å¿«é€’ä¸¢å¤±äº†ï¼Œéœ€è¦æ‚¨æä¾›éªŒè¯ç è¿›è¡Œé€€æ¬¾å¤„ç†", "label": 3},
            {"content": "åˆ·å•å…¼èŒï¼Œæ—¥èµš300å…ƒï¼Œéœ€è¦å«ä»˜æœ¬é‡‘ï¼Œå®Œæˆä»»åŠ¡åç«‹å³è¿”ç°", "label": 1},
            {"content": "æŠ•èµ„ç†è´¢é«˜æ”¶ç›Šï¼Œå¹´åŒ–æ”¶ç›Šç‡20%ï¼Œç¨³èµšä¸èµ”ï¼Œå†…éƒ¨æ¶ˆæ¯", "label": 2},
            {"content": "è¯·é—®è¿™ä¸ªå•†å“ä»€ä¹ˆæ—¶å€™å‘è´§ï¼Ÿç‰©æµä¿¡æ¯æ€ä¹ˆæŸ¥è¯¢ï¼Ÿ", "label": 0},
            {"content": "æ‚¨çš„è´¦æˆ·å­˜åœ¨é£é™©ï¼Œéœ€è¦éªŒè¯èº«ä»½ï¼Œè¯·ç‚¹å‡»é“¾æ¥è¿›è¡Œæ“ä½œ", "label": 3},
            {"content": "ç‚¹èµå…³æ³¨æŠ–éŸ³è´¦å·ï¼Œæ¯æ¡2å…ƒï¼Œæ—¥ç»“å·¥èµ„ï¼Œæ— éœ€æŠ¼é‡‘", "label": 1},
            {"content": "é“¶è¡Œè´·æ¬¾ï¼Œé¢åº¦20ä¸‡ï¼Œåˆ©ç‡ä¼˜æƒ ï¼Œå¿«é€Ÿæ”¾æ¬¾", "label": 2},
            {"content": "æ·˜å®å®¢æœé€šçŸ¥ï¼šæ‚¨çš„è®¢å•å¼‚å¸¸ï¼Œéœ€è¦é‡æ–°ç¡®è®¤æ”¯ä»˜ä¿¡æ¯", "label": 3},
            {"content": "è¿™ä¸ªäº§å“çš„è´¨é‡æ€ä¹ˆæ ·ï¼Ÿæœ‰ä¼˜æƒ æ´»åŠ¨å—ï¼Ÿ", "label": 0},
            {"content": "è‚¡ç¥¨æŠ•èµ„ç¾¤ï¼Œè€å¸ˆå¸¦å•ï¼Œä¿è¯ç›ˆåˆ©ï¼ŒåŠ ç¾¤é¢†å–ç‰›è‚¡", "label": 2}
        ]
        return pd.DataFrame(example_data)

    def human_labeling_session(self, texts, true_labels):
        """äººå·¥æ ‡æ³¨ç¯èŠ‚"""
        print("\n" + "=" * 60)
        print("ğŸ§‘â€ğŸ’» äººå·¥æ ‡æ³¨ç¯èŠ‚")
        print("=" * 60)
        print("0:æ­£å¸¸å¯¹è¯ 1:åˆ·å•è¿”åˆ© 2:è™šå‡æŠ•èµ„ 3:å†’å……å®¢æœ")
        print("è¾“å…¥qé€€å‡ºï¼Œè¾“å…¥ré‡æ–°æŸ¥çœ‹å½“å‰æ–‡æœ¬")
        print("=" * 60)

        human_results = []
        total_human_time = 0

        for i, (text, true_label) in enumerate(zip(texts, true_labels), 1):
            true_category = LABEL_MAP[true_label]

            print(f"\næ ·æœ¬ {i}/{len(texts)}:")
            print(f"åŸæ–‡: {text[:80]}{'...' if len(text) > 80 else ''}")

            start_time = time.time()
            while True:
                try:
                    user_input = input("è¯·è¾“å…¥åˆ†ç±»(0-3): ").strip()

                    if user_input.lower() == 'q':
                        return human_results
                    if user_input.lower() == 'r':
                        print(f"é‡æ–°æ˜¾ç¤º: {text[:80]}{'...' if len(text) > 80 else ''}")
                        continue

                    human_label = int(user_input)
                    if human_label not in LABEL_MAP.keys():
                        print("âŒ è¯·è¾“å…¥0-3")
                        continue

                    process_time = time.time() - start_time
                    total_human_time += process_time

                    human_category = LABEL_MAP[human_label]
                    human_binary = BINARY_MAP[human_category]
                    true_binary = BINARY_MAP[true_category]

                    human_binary_correct = (human_binary == true_binary)
                    human_detailed_correct = (human_label == true_label)

                    result = {
                        'text': text,
                        'true_label': true_label,
                        'true_category': true_category,
                        'true_binary': true_binary,
                        'human_label': human_label,
                        'human_category': human_category,
                        'human_binary': human_binary,
                        'human_time': round(process_time, 3),
                        'human_binary_correct': human_binary_correct,
                        'human_detailed_correct': human_detailed_correct
                    }
                    human_results.append(result)

                    correct_mark = "âœ…" if human_detailed_correct else "âŒ"
                    print(f"æ ‡æ³¨: {human_category} {correct_mark} (çœŸå®: {true_category})")
                    break

                except ValueError:
                    print("âŒ è¾“å…¥æ— æ•ˆ")
                except KeyboardInterrupt:
                    return human_results

        return human_results

    def run_model_predictions(self, human_results):
        """è¿è¡Œæ¨¡å‹é¢„æµ‹"""
        print("\n" + "=" * 60)
        print("ğŸ¤– æ¨¡å‹é¢„æµ‹ç¯èŠ‚")
        print("=" * 60)

        model_results = []
        total_model_time = 0

        for i, human_result in enumerate(human_results, 1):
            print(f"\nè¿›åº¦: {i}/{len(human_results)}")

            start_time = time.time()
            pred_label, pred_category, confidence = self.model_predict(human_result['text'])
            process_time = time.time() - start_time
            total_model_time += process_time

            pred_binary = BINARY_MAP[pred_category] if pred_label != -1 else "é”™è¯¯"
            model_binary_correct = (pred_binary == human_result['true_binary']) if pred_label != -1 else False
            model_detailed_correct = (pred_label == human_result['true_label']) if pred_label != -1 else False

            model_result = human_result.copy()
            model_result.update({
                'model_label': pred_label,
                'model_category': pred_category,
                'model_binary': pred_binary,
                'model_confidence': confidence,
                'model_time': round(process_time, 3),
                'model_binary_correct': model_binary_correct,
                'model_detailed_correct': model_detailed_correct
            })
            model_results.append(model_result)

            human_correct = "âœ…" if human_result['human_detailed_correct'] else "âŒ"
            model_correct = "âœ…" if model_detailed_correct else "âŒ"
            print(f"çœŸå®: {human_result['true_category']}")
            print(f"äººå·¥: {human_result['human_category']} {human_correct}")
            print(f"æ¨¡å‹: {pred_category} {model_correct} ({confidence}%)")

        return model_results

    def calculate_comparison_metrics(self, results):
        """è®¡ç®—å¯¹æ¯”æŒ‡æ ‡"""
        print("\n" + "=" * 60)
        print("ğŸ“Š å‡†ç¡®ç‡ç»Ÿè®¡ç»“æœ")
        print("=" * 60)

        if not results:
            return {}

        total_samples = len(results)

        # äººå·¥å‡†ç¡®ç‡
        human_binary_correct = sum(1 for r in results if r['human_binary_correct'])
        human_detailed_correct = sum(1 for r in results if r['human_detailed_correct'])
        human_binary_accuracy = (human_binary_correct / total_samples) * 100
        human_detailed_accuracy = (human_detailed_correct / total_samples) * 100

        # æ¨¡å‹å‡†ç¡®ç‡
        model_binary_correct = sum(1 for r in results if r['model_binary_correct'])
        model_detailed_correct = sum(1 for r in results if r['model_detailed_correct'])
        model_binary_accuracy = (model_binary_correct / total_samples) * 100
        model_detailed_accuracy = (model_detailed_correct / total_samples) * 100

        # æ—¶é—´ç»Ÿè®¡
        human_times = [r['human_time'] for r in results]
        model_times = [r['model_time'] for r in results]
        avg_human_time = np.mean(human_times)
        avg_model_time = np.mean(model_times)
        speedup_ratio = avg_human_time / avg_model_time if avg_model_time > 0 else 0

        # å„ç±»åˆ«å‡†ç¡®ç‡
        human_category_stats = {}
        model_category_stats = {}
        for category in LABEL_MAP.values():
            human_category_samples = [r for r in results if r['true_category'] == category]
            if human_category_samples:
                human_correct = sum(1 for r in human_category_samples if r['human_detailed_correct'])
                model_correct = sum(1 for r in human_category_samples if r['model_detailed_correct'])

                human_category_stats[category] = {
                    'count': len(human_category_samples),
                    'correct': human_correct,
                    'accuracy': (human_correct / len(human_category_samples)) * 100
                }
                model_category_stats[category] = {
                    'count': len(human_category_samples),
                    'correct': model_correct,
                    'accuracy': (model_correct / len(human_category_samples)) * 100
                }

        # æ‰“å°ç»“æœ
        print(f"æ ·æœ¬æ€»æ•°: {total_samples}")
        print(f"\nğŸ¯ å¤§ç±»å‡†ç¡®ç‡ (æ­£å¸¸vsè¯ˆéª—):")
        print(f"  äººå·¥: {human_binary_accuracy:.1f}% ({human_binary_correct}/{total_samples})")
        print(f"  æ¨¡å‹: {model_binary_accuracy:.1f}% ({model_binary_correct}/{total_samples})")

        print(f"\nğŸ¯ å°ç±»å‡†ç¡®ç‡ (å››åˆ†ç±»):")
        print(f"  äººå·¥: {human_detailed_accuracy:.1f}% ({human_detailed_correct}/{total_samples})")
        print(f"  æ¨¡å‹: {model_detailed_accuracy:.1f}% ({model_detailed_correct}/{total_samples})")

        print(f"\nâ±ï¸  æ•ˆç‡å¯¹æ¯”:")
        print(f"  äººå·¥å¹³å‡ç”¨æ—¶: {avg_human_time:.2f}ç§’/æ¡")
        print(f"  æ¨¡å‹å¹³å‡ç”¨æ—¶: {avg_model_time:.3f}ç§’/æ¡")
        print(f"  åŠ é€Ÿæ¯”: {speedup_ratio:.1f}å€")

        return {
            'total_samples': total_samples,
            'human_binary_accuracy': human_binary_accuracy,
            'human_detailed_accuracy': human_detailed_accuracy,
            'model_binary_accuracy': model_binary_accuracy,
            'model_detailed_accuracy': model_detailed_accuracy,
            'avg_human_time': avg_human_time,
            'avg_model_time': avg_model_time,
            'speedup_ratio': speedup_ratio,
            'human_category_stats': human_category_stats,
            'model_category_stats': model_category_stats,
            'all_results': results
        }

    def plot_final_charts(self, stats):
        """ç»˜åˆ¶æœ€ç»ˆå›¾è¡¨ï¼ˆå¼ºåˆ¶æŒ‡å®šä¸­æ–‡å­—ä½“ï¼‰"""
        os.makedirs("efficiency_test_result", exist_ok=True)

        # å…³é”®ï¼šå¼ºåˆ¶è®¾ç½®å­—ä½“å‚æ•°
        plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']  # å†æ¬¡æ˜ç¡®æŒ‡å®šé»‘ä½“
        plt.rcParams['axes.unicode_minus'] = False  # ç¡®ä¿è´Ÿå·æ­£å¸¸æ˜¾ç¤º
        plt.rcParams['font.size'] = 10  # è°ƒæ•´åŸºç¡€å­—ä½“å¤§å°

        # å›¾è¡¨1: å‡†ç¡®ç‡å¯¹æ¯”
        plt.figure(figsize=(10, 6))

        categories = ['å¤§ç±»å‡†ç¡®ç‡', 'å°ç±»å‡†ç¡®ç‡']
        human_acc = [stats['human_binary_accuracy'], stats['human_detailed_accuracy']]
        model_acc = [stats['model_binary_accuracy'], stats['model_detailed_accuracy']]

        x = np.arange(len(categories))
        width = 0.35

        bars1 = plt.bar(x - width / 2, human_acc, width, label='äººå·¥', color='#2E8B57', alpha=0.8)
        bars2 = plt.bar(x + width / 2, model_acc, width, label='æ¨¡å‹', color='#4682B4', alpha=0.8)

        # æ¯ä¸ªæ–‡æœ¬éƒ½æ‰‹åŠ¨æŒ‡å®šå­—ä½“
        plt.ylabel('å‡†ç¡®ç‡ (%)', fontproperties='SimHei', fontsize=12)
        plt.title('äººå·¥ vs æ¨¡å‹ å‡†ç¡®ç‡å¯¹æ¯”', fontproperties='SimHei', fontsize=14, fontweight='bold')
        plt.xticks(x, categories, fontproperties='SimHei', fontsize=11)
        plt.legend(prop={'family': 'SimHei', 'size': 11})  # å›¾ä¾‹å­—ä½“å•ç‹¬è®¾ç½®
        plt.ylim(0, 105)

        # æ·»åŠ æ•°å€¼æ ‡ç­¾ï¼ˆæŒ‡å®šå­—ä½“ï¼‰
        for bars in [bars1, bars2]:
            for bar in bars:
                height = bar.get_height()
                plt.text(bar.get_x() + bar.get_width() / 2, height + 1,
                         f'{height:.1f}%', ha='center', va='bottom',
                         fontweight='bold', fontproperties='SimHei', fontsize=10)

        plt.tight_layout()
        plt.savefig('efficiency_test_result/accuracy_comparison.png', dpi=300, bbox_inches='tight')
        plt.close()

        # å›¾è¡¨2: æ•ˆç‡å¯¹æ¯”
        plt.figure(figsize=(8, 6))

        labels = ['äººå·¥æ ‡æ³¨', 'æ¨¡å‹é¢„æµ‹']
        times = [stats['avg_human_time'], stats['avg_model_time']]
        colors = ['#FFA07A', '#20B2AA']

        bars = plt.bar(labels, times, color=colors, alpha=0.8)

        # æ¯ä¸ªæ–‡æœ¬æ‰‹åŠ¨æŒ‡å®šå­—ä½“
        plt.ylabel('å¤„ç†æ—¶é—´ (ç§’)', fontproperties='SimHei', fontsize=12)
        plt.title('å¤„ç†æ•ˆç‡å¯¹æ¯”', fontproperties='SimHei', fontsize=14, fontweight='bold')
        plt.xticks(fontproperties='SimHei', fontsize=11)

        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for bar, time_val in zip(bars, times):
            plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 0.01,
                     f'{time_val:.3f}s', ha='center', va='bottom',
                     fontweight='bold', fontproperties='SimHei', fontsize=10)

        plt.tight_layout()
        plt.savefig('efficiency_test_result/efficiency_comparison.png', dpi=300, bbox_inches='tight')
        plt.close()

        print("âœ… å›¾è¡¨å·²ç”Ÿæˆ: efficiency_test_result/accuracy_comparison.png")
        print("âœ… å›¾è¡¨å·²ç”Ÿæˆ: efficiency_test_result/efficiency_comparison.png")

    def generate_final_report(self, results, stats):
        """ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Šï¼ˆåŒ…å«åŸæ–‡ï¼‰"""
        report_path = 'efficiency_test_result/detailed_comparison_report.txt'

        with open(report_path, 'w', encoding='utf-8') as f:
            f.write("=" * 80 + "\n")
            f.write("äººå·¥ vs æ¨¡å‹ å¯¹æ¯”è¯„ä¼°æŠ¥å‘Š\n")
            f.write(f"ç”Ÿæˆæ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write("=" * 80 + "\n\n")

            # æ€»ä½“ç»Ÿè®¡
            f.write("ä¸€ã€æ€»ä½“ç»Ÿè®¡\n")
            f.write("-" * 40 + "\n")
            f.write(f"æ ·æœ¬æ€»æ•°: {stats['total_samples']}\n")
            f.write(
                f"å¤§ç±»å‡†ç¡®ç‡: äººå·¥{stats['human_binary_accuracy']:.1f}% vs æ¨¡å‹{stats['model_binary_accuracy']:.1f}%\n")
            f.write(
                f"å°ç±»å‡†ç¡®ç‡: äººå·¥{stats['human_detailed_accuracy']:.1f}% vs æ¨¡å‹{stats['model_detailed_accuracy']:.1f}%\n")
            f.write(
                f"å¤„ç†æ•ˆç‡: äººå·¥{stats['avg_human_time']:.2f}s vs æ¨¡å‹{stats['avg_model_time']:.3f}s (åŠ é€Ÿ{stats['speedup_ratio']:.1f}å€)\n\n")

            # å„ç±»åˆ«ç»Ÿè®¡
            f.write("äºŒã€å„ç±»åˆ«å‡†ç¡®ç‡\n")
            f.write("-" * 40 + "\n")
            for category in LABEL_MAP.values():
                if category in stats['human_category_stats']:
                    human_stats = stats['human_category_stats'][category]
                    model_stats = stats['model_category_stats'][category]
                    diff = model_stats['accuracy'] - human_stats['accuracy']
                    f.write(
                        f"{category}: äººå·¥{human_stats['accuracy']:.1f}% vs æ¨¡å‹{model_stats['accuracy']:.1f}% (å·®å¼‚{diff:+.1f}%)\n")
            f.write("\n")

            # è¯¦ç»†æ ·æœ¬æ•°æ®ï¼ˆåŒ…å«åŸæ–‡ï¼‰
            f.write("ä¸‰ã€è¯¦ç»†æ ·æœ¬æ•°æ®\n")
            f.write("-" * 120 + "\n")
            header = f"{'åºå·':<3} {'çœŸå®ç±»åˆ«':<8} {'äººå·¥æ ‡æ³¨':<8} {'æ¨¡å‹é¢„æµ‹':<8} {'äººå·¥æ­£ç¡®':<6} {'æ¨¡å‹æ­£ç¡®':<6} {'åŸæ–‡æ‘˜è¦':<40}\n"
            f.write(header)
            f.write("-" * 120 + "\n")

            for i, r in enumerate(results, 1):
                human_correct = "âœ…" if r['human_detailed_correct'] else "âŒ"
                model_correct = "âœ…" if r['model_detailed_correct'] else "âŒ"
                text_preview = r['text'][:35] + "..." if len(r['text']) > 35 else r['text']

                line = f"{i:<3} {r['true_category']:<8} {r['human_category']:<8} {r['model_category']:<8} {human_correct:<6} {model_correct:<6} {text_preview:<40}\n"
                f.write(line)

            f.write("\n" + "=" * 100 + "\n")
            f.write("å››ã€å®Œæ•´åŸæ–‡å†…å®¹\n")
            f.write("=" * 100 + "\n")

            for i, r in enumerate(results, 1):
                f.write(f"\nã€æ ·æœ¬ {i}ã€‘\n")
                f.write(f"çœŸå®ç±»åˆ«: {r['true_category']}\n")
                f.write(f"äººå·¥æ ‡æ³¨: {r['human_category']}\n")
                f.write(f"æ¨¡å‹é¢„æµ‹: {r['model_category']} (ç½®ä¿¡åº¦: {r['model_confidence']}%)\n")
                f.write(f"äººå·¥ç”¨æ—¶: {r['human_time']:.2f}s | æ¨¡å‹ç”¨æ—¶: {r['model_time']:.3f}s\n")
                f.write(f"åŸæ–‡: {r['text']}\n")
                f.write("-" * 80 + "\n")

        print(f"ğŸ“„ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜è‡³: {report_path}")


def main():
    """ä¸»å‡½æ•°"""
    # é…ç½®è·¯å¾„
    model_path = "mini_bert_scam_model/best_model"
    tokenizer_path = "mini_bert_scam_model/best_tokenizer"

    # åˆå§‹åŒ–è¯„ä¼°ç³»ç»Ÿ
    evaluator = HumanVsModelFinal(model_path, tokenizer_path)

    # åŠ è½½æ¨¡å‹
    if not evaluator.load_model():
        return

    # åŠ è½½æ•°æ®
    df = evaluator.load_data_with_true_labels()
    sample_size = min(10, len(df))

    test_data = df.sample(n=sample_size, random_state=42)
    texts = test_data['content'].tolist()
    true_labels = test_data['label'].tolist()

    print(f"\næµ‹è¯•æ ·æœ¬: {sample_size}æ¡")

    # äººå·¥æ ‡æ³¨
    human_results = evaluator.human_labeling_session(texts, true_labels)
    if not human_results:
        print("âŒ äººå·¥æ ‡æ³¨æœªå®Œæˆ")
        return

    # æ¨¡å‹é¢„æµ‹
    model_results = evaluator.run_model_predictions(human_results)

    # ç»Ÿè®¡åˆ†æ
    stats = evaluator.calculate_comparison_metrics(model_results)

    # ç”Ÿæˆå›¾è¡¨å’ŒæŠ¥å‘Š
    evaluator.plot_final_charts(stats)
    evaluator.generate_final_report(model_results, stats)

    print("\n" + "=" * 60)
    print("ğŸ‰ è¯„ä¼°å®Œæˆï¼")
    print("ğŸ“Š 2ä¸ªå›¾è¡¨: efficiency_test_result/ ç›®å½•")
    print("ğŸ“„ è¯¦ç»†æŠ¥å‘Š: efficiency_test_result/detailed_comparison_report.txt")
    print("=" * 60)


if __name__ == "__main__":
    main()
